name: Process Weather Data

on:
  schedule:
    # Run every 6 hours at 00:30, 06:30, 12:30, 18:30 UTC
    - cron: '30 */6 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write

jobs:
  process-grib-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y gdal-bin python3-gdal curl jq
          
          # Verify GDAL installation
          gdalinfo --version
          
      - name: Get current GFS run info
        id: gfs-info
        run: |
          # Get current date and determine the latest available GFS run
          # GFS data is usually available 3-4 hours after model run time
          CURRENT_HOUR=$(date -u +%H)
          
          # Account for GFS data availability delay (usually 3-4 hours)
          if [ $CURRENT_HOUR -ge 21 ]; then
            GFS_RUN="18"
            GFS_DATE=$(date -u +%Y%m%d)
          elif [ $CURRENT_HOUR -ge 15 ]; then
            GFS_RUN="12" 
            GFS_DATE=$(date -u +%Y%m%d)
          elif [ $CURRENT_HOUR -ge 9 ]; then
            GFS_RUN="06"
            GFS_DATE=$(date -u +%Y%m%d)
          elif [ $CURRENT_HOUR -ge 3 ]; then
            GFS_RUN="00"
            GFS_DATE=$(date -u +%Y%m%d)
          else
            # Use previous day's 18Z run
            GFS_RUN="18"
            GFS_DATE=$(date -u -d "yesterday" +%Y%m%d)
          fi
          
          echo "gfs_date=$GFS_DATE" >> $GITHUB_OUTPUT
          echo "gfs_run=$GFS_RUN" >> $GITHUB_OUTPUT
          echo "Processing GFS data for date: $GFS_DATE, run: ${GFS_RUN}Z"

      - name: Download GFS GRIB data
        run: |
          GFS_DATE=${{ steps.gfs-info.outputs.gfs_date }}
          GFS_RUN=${{ steps.gfs-info.outputs.gfs_run }}
          
          # Create directory for raw data
          mkdir -p raw-data
          cd raw-data
          
          echo "Downloading current GFS data for ${GFS_DATE}/${GFS_RUN}Z"
          
          # Function to try downloading with fallback to previous runs
          download_with_fallback() {
            local var_name=$1
            local lev_name=$2
            local output_file=$3
            local current_date=$GFS_DATE
            local current_run=$GFS_RUN
            
            # Try current run first
            for attempt in 1 2 3; do
              echo "Attempt $attempt: Downloading ${var_name} for ${current_date}/${current_run}Z..."
              
              if curl -f -o "$output_file" \
                "https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p25.pl?file=gfs.t${current_run}z.pgrb2.0p25.f000&var_${var_name}=on&lev_${lev_name}=on&dir=%2Fgfs.${current_date}%2F${current_run}%2Fatmos"; then
                echo "Successfully downloaded ${var_name}"
                return 0
              fi
              
              # Fallback to previous run
              if [ "$current_run" = "18" ]; then
                current_run="12"
              elif [ "$current_run" = "12" ]; then
                current_run="06"
              elif [ "$current_run" = "06" ]; then
                current_run="00"
              else
                # Go to previous day's 18Z
                current_date=$(date -u -d "${current_date:0:4}-${current_date:4:2}-${current_date:6:2} - 1 day" +%Y%m%d)
                current_run="18"
              fi
              
              echo "Trying fallback: ${current_date}/${current_run}Z"
            done
            
            echo "Failed to download ${var_name} after all attempts"
            return 1
          }
          
          # Download U component of wind at 10m above ground
          download_with_fallback "UGRD" "10_m_above_ground" "wind_u_current.grib"
          
          # Download V component of wind at 10m above ground  
          download_with_fallback "VGRD" "10_m_above_ground" "wind_v_current.grib"
          
          # List downloaded files
          echo "Files downloaded:"
          ls -la *.grib 2>/dev/null || echo "No GRIB files downloaded"

      - name: Process GRIB to WeatherLayers format
        run: |
          cd raw-data
          GFS_DATE=${{ steps.gfs-info.outputs.gfs_date }}
          GFS_RUN=${{ steps.gfs-info.outputs.gfs_run }}
          
          # Check if wind files exist
          if [[ -f "wind_u_current.grib" && -f "wind_v_current.grib" ]]; then
            echo "Processing wind data..."
            
            # Get info about the GRIB files
            echo "Wind U info:"
            gdalinfo "wind_u_current.grib"
            echo "Wind V info:"  
            gdalinfo "wind_v_current.grib"
            
            # Create VRT (Virtual Dataset) combining U and V components
            # Note: Third band duplicates V for proper RGBA structure (weatherlayers-gl uses RG channels)
            gdalbuildvrt -separate "wind_current.vrt" "wind_u_current.grib" "wind_v_current.grib" "wind_v_current.grib"
            
            # Convert to PNG format for weatherlayers-gl
            # WeatherLayers expects: R=U component, G=V component, scaled to [0,255] with 128 as zero
            # Scale from [-30, 30] m/s to [0, 255] range (128 = 0 m/s)
            gdal_translate -ot Byte -scale -30 30 0 255 -of PNG "wind_current.vrt" "../processed-data/wind_${GFS_DATE}_${GFS_RUN}z.png"
            
            echo "Wind data processed successfully: wind_${GFS_DATE}_${GFS_RUN}z.png"
          else
            echo "Wind GRIB files not found, skipping wind processing"
          fi

      - name: Generate metadata
        run: |
          GFS_DATE=${{ steps.gfs-info.outputs.gfs_date }}
          GFS_RUN=${{ steps.gfs-info.outputs.gfs_run }}
          
          # Create metadata JSON
          cat > processed-data/metadata_${GFS_DATE}_${GFS_RUN}z.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "gfs_date": "${GFS_DATE}",
            "gfs_run": "${GFS_RUN}",
            "forecast_hour": "000",
            "bounds": [-180, -90, 180, 90],
            "layers": {
              "wind": {
                "file": "wind_${GFS_DATE}_${GFS_RUN}z.png",
                "type": "vector",
                "units": "m/s",
                "range": [-30, 30],
                "encoding": "RG channels, 128=0 m/s, scale 0-255"
              }
            }
          }
          EOF
          
          # List processed files
          echo "Generated files:"
          ls -la processed-data/

      - name: Clean up old processed data (keep last 7 days)
        run: |
          echo "Cleaning up files older than 7 days in processed-data folder..."
          
          # Create processed-data directory if it doesn't exist
          mkdir -p processed-data
          
          # Show current files before cleanup
          echo "Files before cleanup:"
          find processed-data/ -type f | sort
          
          # Remove PNG files older than 7 days
          DELETED_PNG=$(find processed-data/ -name "*.png" -mtime +7 -exec rm -v {} \; | wc -l)
          echo "Deleted $DELETED_PNG PNG files older than 7 days"
          
          # Remove JSON metadata files older than 7 days
          DELETED_JSON=$(find processed-data/ -name "*.json" -mtime +7 -exec rm -v {} \; | wc -l)
          echo "Deleted $DELETED_JSON JSON files older than 7 days"
          
          # Remove any other files older than 7 days (catch-all)
          DELETED_OTHER=$(find processed-data/ -type f ! -name "*.png" ! -name "*.json" -mtime +7 -exec rm -v {} \; | wc -l)
          echo "Deleted $DELETED_OTHER other files older than 7 days"
          
          # Clean up temporary raw data (always delete these)
          if [ -d "raw-data" ]; then
            echo "Cleaning up raw-data directory..."
            rm -rf raw-data/
            echo "Raw data directory removed"
          fi
          
          # Show remaining files after cleanup
          echo "Files after cleanup:"
          find processed-data/ -type f | sort
          
          echo "Cleanup completed. Total files deleted: $((DELETED_PNG + DELETED_JSON + DELETED_OTHER))"

      - name: Commit and push processed data
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add processed files
          git add processed-data/
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update weather data: $(date -u +%Y-%m-%d_%H:%M)Z"
            git push
          fi

      - name: Commit cleanup changes
        run: |
          # Check if cleanup resulted in file deletions that need to be committed
          if ! git diff --exit-code processed-data/; then
            echo "Committing cleanup changes..."
            git config --local user.email "action@github.com"
            git config --local user.name "GitHub Action"
            git add processed-data/
            git commit -m "Clean up old weather data (keep 7 days) - $(date -u +%Y-%m-%d_%H:%M)Z"
            git push
            echo "Cleanup changes committed and pushed"
          else
            echo "No cleanup changes to commit"
          fi
